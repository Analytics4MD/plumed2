/**

\page Analysis Analysis

\section analysisbas Introduction

A molecular dynamics trajectory is in essence an ordered set of configurations of atoms.  Trajectory
analysis algorithms are methods that allow us to extract meaningful information from this extremely
high-dimensionality information.  In extracting this information much of the information in the trajectory
will be discarded and assumed to be irrelevant to the problem at hand.  For example, when we calculate a
histogram from a trajectory we throw away all information on the order the frames were visited during the
trajectory.  We instead opt to display a time average that shows the parts of configuration space that were  
visited most frequently.  There are many situations in which this is a reasonable thing to do as we know that
time averages are equivalent to ensemble averages in the long timescale limit and that these average probabilities
of being in different parts of configuration space, \f$P(s)\f$, are thus related to the underlying free
energy, \f$F(s)\f$, via:
\f[
F(s) = - k_B T \ln P(s)
\f]
In fact we can even exploit our understanding of statistical thermodynamics and basic algebra and derive expressions
that connect the time average of the probability of being in a particular configuration in a simulaton at one temperature,
\f$T_1\f$, with the free energy at a second different temperature, \f$T_2\f$:

\f[
P(s',t) = \frac{ \sum_{t'=0}^t \delta( s(x) - s' ) \exp\left( +( \left[\frac{1}{T_1} - \frac{1}{T_2}\right] \frac{U(x,t')}{k_B} \right) }{ \sum_{t'=0}^t \exp\left( +\left[\frac{1}{T_1} - \frac{1}{T_2}\right] \frac{U(x,t')}{k_B} \right) }
\f]

Similarly, we can exploit statistical thermodynamics to calculate the free energy of a configuration from the probability
of being in that configuration that was calculated in an biased simulation.

\f[
P(s',t) = \frac{ \sum_{t'=0}^t \delta( s(x) - s' ) \exp\left( +\frac{V(x,t')}{k_B T} \right) }{ \sum_{t'=0}^t \exp\left( +\frac{V(x,t')}{k_B T} \right) }
\f]

In both of these expressions we have a number of frames from a trajectory of length \f$t'\f$.  In the first of these expressions
\f$U(x,t')\f$ is the potential energy of the system (including any biases) at time \f$t'\f$.  Meanwhile, in the second of
these expressions \f$V(x,t')\f$ is the value of the simulation bias at time \f$t'\f$.  The dirac delta function, \f$\delta\f$,
on the first lines of these expressions indicates that we are going to calculate a \ref HISTOGRAM as a function of some
collective variable \f$s(x)\f$ that can be calulated from the atomic positions \f$x\f$.  Herein lies the central problem:
what collective variable function should we use in this expression?  What \f$s(x)\f$ will give us the most meaningful
interpretation of the trajectory data?  Obviously, we can use any one of the \ref colvarintro implemented in tandem
with \ref HISTOGRAM.  We can also, however, use a number of other algorithms that essentially ``learn" from our simulation
trajectory and that thus provide ways of alternative ways of displaying the probability information that can be calculated
using the various formula above.  These techniques are described in the following sections.

\section basanal Basic analysis

PLUMED contains a suite of tools that can be used to analyse simulation trjaectories.
These tools can be employed on the fly during an MD run or through driver using the 
\ref driver tool.  The simplest of these tools are:

<table align=center frame=void width=95%% cellpadding=5%%>
<tr> <td width=5%> \subpage PRINT </td> <td>Print quantities to a file.</td> </tr>
<tr> <td width=5%> \subpage DUMPATOMS </td> <td>Dump selected atoms on a file.</td> </tr>
</table>

These allow you to print colvar values or the positions of atoms to a file. 

A more complex way of analysing the data in your trajectory is to calculate a histogram
as function of a small number of collective variables.  Alternatively, by running a large 
number of simulations you can calculate committor probabilities.  The commands for doing these 
sorts of analyses within PLUMED are as follows:

<table align=center frame=void width=95%% cellpadding=5%%>
<tr> <td width=5%> \subpage HISTOGRAM </td> <td>Calculate the probability density as a function of a few CVs either using kernel density estimation, or a discretehistogram estimation. </td> </tr>
<tr> <td width=5%> \subpage COMMITTOR </td> <td>Does a committor analysis.</td> </tr> 
</table>

PLUMED then has a number of other tools for doing more sophisticated forms of analysis that are 
described in the sections that follow.

\section diag Diagnostic tools

PLUMED has a number of diagnostic tools that can be used to check that new Actions are working correctly: 

<table align=center frame=void width=95%% cellpadding=5%%>
<tr> <td width=5%> \subpage DUMPFORCES </td> <td>Dump the force acting on one of a values in a file.  </td> </tr>
<tr> <td width=5%> \subpage DUMPDERIVATIVES </td> <td>Dump the derivatives with respect to the input parameters for one or more objects (generally CVs, functions or biases).</td> </tr>
<tr> <td width=5%> \subpage DUMPMASSCHARGE </td> <td>Dump masses and charges on a selected file.</td> </tr>
<tr> <td width=5%> \subpage DUMPPROJECTIONS </td> <td>Dump the derivatives with respect to the input parameters for one or more objects (generally CVs, functions or biases).</td> </tr>
</table>

These commands allow you to test that derivatives and forces are calculated correctly
within colvars and functions.  One place where this is very useful is when you are testing whether or
not you have implemented the derivatives of a new collective variables correctly.  So for example if
we wanted to do such a test on the distance CV we would employ an input file something like this:

\verbatim
d1: DISTANCE ATOMS=1,2
d1n: DISTANCE ATOMS=1,2 NUMERICAL_DERIVATIVES
DUMPDERIVATIVES ARG=d1,d1n FILE=derivatives
\endverbatim

The first of these two distance commands calculates the analytical derivtives of the distance
while the second calculates these derivatives numerically.  Obviously, if your CV is implemented
correctly these two sets of quantities should be nearly identical.

\section dissimilaritym Calculating dissimilarity matrices

One of the simplest things that we can do with a trajectory is that we can calculate the dissimilarity between 
every pair of frames within it.  When using the \ref dimred "dimensionality reduction" algorithms described in 
the sections that follow the first step is to calculate this matrix.  Consequently, within PLUMED the following 
command will collect the trajectory data as your simulation progressed and calculate the dissimilarities: 

<table align=center frame=void width=95%% cellpadding=5%%>
<tr> <td width=5%> \subpage EUCLIDEAN_DISSIMILARITIES </td> <td> Calculate the matrix of dissimilarities between a trajectory of atomic configurations. </td> </tr>
</table>

By exploiting the functionality described in \ref dists you can calculate these dissimilarities in
a wide variety of different ways (e.g. you can use \ref RMSD, or you can use a collection of collective variable
values see \ref TARGET).  If you wish to view this dissimilarity information you can print these quantities 
to a file using:

<table align=center frame=void width=95%% cellpadding=5%%>
<tr> <td width=5%> \subpage PRINT_DISSIMILARITY_MATRIX </td> <td> Print the matrix of dissimilarities between a trajectory of atomic configurations. </td> </tr>
</table>

In addition, if PLUMED does not calculate the dissimilarities you need you can read this information from an 
external file

<table align=center frame=void width=95%% cellpadding=5%%>
<tr> <td width=5%> \subpage READ_DISSIMILARITY_MATRIX </td> <td> Read a matrix of dissimilarities between a trajectory of atomic configurations from a file. </td> </tr>
</table>
 
N.B. You can only use the \ref READ_DISSIMILARITY_MATRIX command when you are doing post-processing.  If you have a 
trajectory that contains the configurations you read in the configurations from the trajectory using 
\ref READ_ANALYSIS_FRAMES 

\ref landmarks Landmark Selection

Many of the techniques described in the following sections are very computationally expensive to run on large trajectories.
A common strategy is thus to use a landmark selection algorithm to pick a particularly-reprentative subset of trajectory
frames and to only apply the expensive analysis algorithm on these configurations.  The various landmark selection algorithms
that are available in PLUMED are as follows

@LANDMARKS@

Some of these algorithms (e.g. \ref LANDMARK_SELECT_STRIDE) can collect data from the trajectory themselves.  Others such as
\ref LANDMARK_SELECT_FPS must take a dissimilarity matrix action as input.  That is to say they must be used as follows:

\verbatim
ss1: EUCLIDEAN_DISSIMILARITIES STRIDE=1 USE_ALL_DATA ARG=d1
ll2: LANDMARK_SELECT_FPS USE_OUTPUT_DATA_FROM=ss1 NLANDMARKS=300
\endverbatim

When landmark selection is performed in this way a weight is ascribed to each of the landmark configurations.  This weight is
calculated by summing the weights of all the trajectory frames in each of the landmarks Voronoi polyhedra 
(https://en.wikipedia.org/wiki/Voronoi_diagram).  The weight of each trajectory frame is one unless you are reweighting using the
formula described in the \ref analysisbas to counteract the fact of a simulation bias or an elevated temperature.  If you are reweighting
using these formula the weight of each of the points is equal to the exponential term in the numerator of these expressions.

\section dimred Dimensionality Reduction

Many dimensionality reduction algorithms work in a manner similar to the way we use when we make maps. You start with distances 
between London, Belfast, Paris and Dublin and then you try to arrange points on a piece of paper so that the (suitably transformed) 
distances between the points in your map representing each of those cities are related to the true distances between the cities.  
Stating this more mathematically MDS endeavors to find an <a href="http://en.wikipedia.org/wiki/Isometry">isometry</a> 
between points distributed in a high-dimensional space and a set of points distributed in a low-dimensional plane.  
In other words, if we have \f$M\f$ \f$D\f$-dimensional points, \f$\mathbf{X}\f$, 
and we can calculate dissimilarities between pairs them, \f$D_{ij}\f$, we can, with an MDS calculation, try to create \f$M\f$ projections, 
\f$\mathbf{x}\f$, of the high dimensionality points in a \f$d\f$-dimensional linear space by trying to arrange the projections so that the 
Euclidean distances between pairs of them, \f$d_{ij}\f$, resemble the dissimilarities between the high dimensional points.  In short we minimize:

\f[
\chi^2 = \sum_{i \ne j} w_i w_j \left( F(D_{ij}) - f(d_{ij}) \right)^2
\f]

where \f$F(D_{ij})\f$ is some transformation of the distance between point \f$X^{i}\f$ and point \f$X^{j}\f$ and \f$f(d_{ij})\f$ is some transformation
of the distance between the projection of \f$X^{i}\f$, \f$x^i\f$, and the projection of \f$X^{j}\f$, \f$x^j\f$.  \f$w_i\f$ and \f$w_j\f$ are the weights
of configurations \f$X^i\f$ and \f$^j\f$ respectively.  These weights are caclulated using the reweighting and voronoi polyhedra approaches described in
previous sections.  A tutorial on dimensionality reduction and how it can be used to analyse simulations can be found in the tutorial \ref belfast-3 and in 
the following <a href="https://www.youtube.com/watch?v=ofC2qz0_9_A&feature=youtu.be" > short video.</a>

Within PLUMED running an input to run a dimensionality reduction algorithm can be as simple as:

\verbatim
ss1: EUCLIDEAN_DISSIMILARITIES STRIDE=1 USE_ALL_DATA ARG=d1
mds: CLASSICAL_MDS USE_OUTPUT_DATA_FROM=ss1 NLOW_DIM=2
\endverbatim

Where we have to use the \ref EUCLIDEAN_DISSIMILARITIES action here in order to calculate the matrix of dissimilarities between trajectory frames.
We can even throw some landmark selection into this procedure and perform

\verbatim
ss1: EUCLIDEAN_DISSIMILARITIES STRIDE=1 USE_ALL_DATA ARG=d1
ll2: LANDMARK_SELECT_FPS USE_OUTPUT_DATA_FROM=ss1 NLANDMARKS=300
mds: CLASSICAL_MDS USE_OUTPUT_DATA_FROM=ll2 NLOW_DIM=2
osample: PROJECT_ALL_ANALYSIS_DATA USE_OUTPUT_DATA_FROM=ss1 PROJECTION=smap
\endverbatim

Notice here that the final command allows us to caluclate the projections of all the non-landmark points that were collected by the action with
label ss1.

Dimensionality can be more complicated, however, because the stress function that calculates \f$\chi^2\f$ has to optimised rather carefully using
a number of different algorithms.  The various algorithms that can be used to optimise this function are described below

@DIMRED@

\ref output Outputting the results from analysis algorithms

The following methods are available for printing the result output by the various analysis algorithms:

<table align=center frame=void width=95%% cellpadding=5%%>
<tr> <td width=5%> \subpage OUTPUT_ANALYSIS_DATA_TO_COLVAR </td> <td> Output the results from an analysis using the PLUMED colvar file format. </td> </tr>
<tr> <td width=5%> \subpage OUTPUT_ANALYSIS_DATA_TO_PDB </td> <td> Output the results from an analysis using the PDB file format.</td> </tr>
</table>

If you use the above commands to output data from one of the \ref landmarks algorithms then only the second will give you information on the 
atomic positions in your landmark configurations and their associated weights.  The first of these commands will give the values of the colvars
in the landmark configurations only.  If you use the above commands to output data from one of the \ref dimred algorithms then 
\ref OUTPUT_ANALYSIS_DATA_TO_COLVAR will give you an output file that contains the projection for each of your input points.  \ref OUTPUT_ANALYSIS_DATA_TO_PDB
will give you a PDB that contains the position of the input point, the projections and the weight of the configuration.

A nice feature of plumed is that when you use \ref landmarks algorithms or \ref dimred algorithms the output information is just a vector of 
variables.  As such you can use \ref HISTOGRAM to construct a histogram of the information generated by these algorithms.


*/

